---
title: Parallel Programming Hw4 Report

---

# Parallel Programming Hw4 Report 
### 11208530余雪淩
(需要在淺色模式下閱讀，圖表的文字才看得到)
## 1. Implemention

### **a. Flash Attention**

Flash Attention的目標是高效計算自注意力機制中的公式：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q \cdot K^T}{\sqrt{d}}\right) \cdot V $$

- Q、K、V 分別是Query、Key和Value的矩陣，大小為N*d，其中N為序列長度，d是維度。
- 主要困難點在於計算 $Q \cdot K^T$時，其複雜度為 $O(N^2 \cdot d)$，並進一步通過 softmax 和矩陣乘法與 V相乘。此步驟在N和d增大時會產生龐大的計算負擔。

本程式主要分為以下幾個步驟：

1. 計算Q和K的內積並進行縮放。
2. 計算每行的最大值並用於數值穩定的指數運算。
3. 減去最大值，計算指數，並normalize以得到矩陣P。
4. 將P和V結合，計算最後的輸出O。

### **b. Matrix Blocking**

- Q和K的大小皆為N\*d，產生的$S = Q \cdot K^T$的大小則為N\*N。
- 將Q、K分成16\*16 ($B_r \times B_c$) 的block。
* 每個thread block 負責計算一個$B_r \times B_c$。

Q、K每32行會被劃分為一個tile，每個tile計算完Q、K的內積後，結果對應至S中的一部分子矩陣。

### **c. Implementation**

#### (1) **Scaled Dot Product**：
   - 在 `QKDotAndScalarKernel` 中，計算Q和K的縮放內積：
     $$
     S_{ij} = \frac{1}{\sqrt{d}} \cdot (Q \cdot K^T)
     $$
   - 使用Share Memory暫存Q、K的計算結果，每個 thread 負責計算該區塊中的一個元素，並透過Share Memory進行內積的累加。
   - grid設為$\lceil N / 16 \rceil \times \lceil N / 16 \rceil$，每個block負責16\*16的計算，每個threads負責計算一筆資料。
```clike
dim3 grid_qk_dot((N + block_qk_dot.x - 1) / block_qk_dot.x,
    (N + block_qk_dot.y - 1) / block_qk_dot.y);
```
```clike
__shared__ float q_shared[32 * 32];
__shared__ float k_shared[32 * 32];

for (int tile = 0; tile < (d + blockDim.x - 1) / blockDim.x; tile++) {
    if (row < N && tile * blockDim.x + ty < d) {
        q_shared[shared_idx] = q[row * d + tile * blockDim.x + ty];
    } else {
        q_shared[shared_idx] = 0.0f;
    }
    if (col < N && tile * blockDim.x + tx < d) {
        k_shared[shared_idx] = k[col * d + tile * blockDim.x + tx];
    } else {
        k_shared[shared_idx] = 0.0f;
    }
    __syncthreads();
    for (int t = 0; t < blockDim.x; t++) {
        sum += q_shared[tx * blockDim.x + t] * k_shared[t * blockDim.x + ty];
    }
    __syncthreads();
}
out[row * N + col] = sum * scalar;

```

#### (2) **Numerical Stabilization**：
   - 將上一步驟算出來的S傳至`RowMaxKernel` 中計算每行的最大值 **$m_i$**，並在 `MinusMaxAndExpKernel` 中將每個元素減去最大值後再計算指數值：$P_{ij} = e^{S_{ij} - m[i]}$
   - 減去最大值的操作可以避免指數函數運算時數值過大或溢出。
* `RowMaxKernel`的grid配置為 $\lceil N / 32 \rceil \times 1$，block為$32 \times 1$，每個 thread block 負責計算一行。

```clike
__global__ void RowMaxKernel(float *out, float *in, int N, int stride) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N) {
        float max_val = -FLT_MAX;
        for (int col = 0; col < stride; col++) {
            max_val = fmaxf(max_val, in[row * stride + col]);
        }
        out[row] = max_val;
    }
}
```

#### (3) **Row Sum and Normalization**：
   - 在 `RowSumKernel` 中計算每行的指數總和 **$l_i$**，並使用該總和進行 Softmax：
     $$
     \text{Softmax}(S_{ij}) = \frac{e^{S_{ij} - m[i]}}{\ell[i]}
     $$
     
*    grid配置為 $\lceil N / 32 \rceil \times 1$，block為$32 \times 1$
```clike
__global__ void RowSumKernel(float *out, float *in, int N, int stride) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    if (row < N) {
        float sum = 0.0f;
        for (int col = 0; col < stride; col++) {
            sum += in[row * stride + col];
        }
        out[row] = sum;
    }
}
```
#### (4) **Output Update**：
   - 在 `UpdateOutputKernel` 中，根據 Softmax 權重$P_{ij}$和$V$計算最終輸出：
     $$
     O_{ij} = \sum_k P_{ik} \cdot V_{kj}
     $$
* 為避免除以零，設置每行總和 **$l_i$** 下限為1e−8。
```clike
__global__ void UpdateOutputKernel(float *out, float *q, float *k, float *v, float *row_sum, int N, int d) {
    int row = blockIdx.x * blockDim.x + threadIdx.x;
    int col = blockIdx.y * blockDim.y + threadIdx.y;
    if (row < N && col < d) {
        float result = 0.0f;
        for (int t = 0; t < N; t++) {
            result += q[row * N + t] * v[t * d + col] / fmaxf(row_sum[row], 1e-8f);
        }
        out[row * d + col] = result;
    }
}
```




---
## **d. Configuration Specifications & Justifications**
```text
Device : "NVIDIA GeForce GTX 1080"
Total amount of constant memory:               65536 bytes
Total amount of shared memory per block:       49152 bytes
Total number of registers available per block: 65536
Warp size:                                     32
Maximum number of threads per multiprocessor:  2048
Maximum number of threads per block:           1024
Max dimension size of a thread block (x,y,z): (1024, 1024, 64)
Max dimension size of a grid size    (x,y,z): (2147483647, 65535, 65535)
```
* 考量到GPU的限制，本程式中的block size大多使用32*32，剛好符合Maximum number of threads per block = 1024的要求，將運算資源利用達到最大化。
* 由於`QKDotAndScalarKernel`是最消耗運算資源的部分，起初用32*32的配置一直無法過第22筆測資(TLE)，仔細計算後會發現：
    * N的最大值為32768，在這種情況下總block數達到1024×1024=1,048,576
    * 使用到的share memory大小為2\*32\*32*4(float) = 8192 bytes
    * 一個 SM 可以容納的 Block 數為：$\frac{49152 bytes}{8192 bytes} = 6$ blocks
    * 在總blocks數量龐大的情況下，會有較多blocks無法被SM同時處理，等待時間可能變長，所以選擇將block size調小至16*16，經過這樣的調整後，成功通過一直TLE的第22筆測資。
    
以下為這次使用的grid、block配置，主要將2d block一律調整為16\*16。
 
```clike
dim3 block_qk_dot(16, 16);
dim3 grid_qk_dot(N/16, N/16);

dim3 block_1d_row_max(32, 1);
dim3 grid_1d_row_max(N/32, 1);

dim3 block_minus_max(32, 32);
dim3 grid_minus_max(N/32, N/32);

dim3 block_1d_row_sum(32, 1);
dim3 grid_1d_row_sum(N/32, 1);

dim3 block_update_output(16, 16);
dim3 grid_update_output(N/16, N/16);
```
---

## **2. Profiling Results**
考量到profiling時間，本次使用t25測資



| **Kernel**                                    | **Metric Description**        | **Min**       | **Max**       | **Avg**       |
|-----------------------------------------------|--------------------------------|---------------|---------------|---------------|
| **RowMaxKernel**                              | Achieved Occupancy            | 0.192347      | 0.193096      | 0.192810      |
|                                               | Multiprocessor Activity       | 98.11%        | 98.69%        | 98.46%        |
| **QKDotAndScalarKernel**                      | Achieved Occupancy            | 0.985199      | 0.985306      | 0.985251      |
|                                               | Multiprocessor Activity       | 99.97%        | 99.98%        | 99.98%        |
| **UpdateOutputKernel**                        | Achieved Occupancy            | 0.989403      | 0.993768      | 0.992632      |
|                                               | Multiprocessor Activity       | 93.62%        | 93.83%        | 93.70%        |
| **MinusMaxAndExpKernel**                      | Achieved Occupancy            | 0.789999      | 0.792142      | 0.791121      |
|                                               | Multiprocessor Activity       | 99.93%        | 99.94%        | 99.93%        |
| **RowSumKernel**                              | Achieved Occupancy            | 0.192372      | 0.193146      | 0.192705      |
|                                               | Multiprocessor Activity       | 97.92%        | 98.63%        | 98.34%        |

### Global and Shared Memory Metrics

| **Kernel**                                    | **Metric Description**              | **Min**       | **Max**       | **Avg**       |
|-----------------------------------------------|--------------------------------------|---------------|---------------|---------------|
| **RowMaxKernel**                              | Global Load Throughput              | 320.61GB/s    | 323.27GB/s    | 322.08GB/s    |
|                                               | Global Store Throughput             | 5.0095MB/s    | 5.0511MB/s    | 5.0326MB/s    |
| **QKDotAndScalarKernel**                      | Global Load Throughput              | 137.41GB/s    | 140.43GB/s    | 139.21GB/s    |
|                                               | Global Store Throughput             | 109.92GB/s    | 112.34GB/s    | 111.37GB/s    |
|                                               | Shared Memory Load Throughput       | 3957.3GB/s    | 4044.3GB/s    | 4009.4GB/s    |
|                                               | Shared Memory Store Throughput      | 439.70GB/s    | 449.37GB/s    | 445.49GB/s    |
| **UpdateOutputKernel**                        | Global Load Throughput              | 827.49GB/s    | 840.87GB/s    | 833.76GB/s    |
|                                               | Global Store Throughput             | 194.70MB/s    | 197.85MB/s    | 196.17MB/s    |
| **MinusMaxAndExpKernel**                      | Global Load Throughput              | 330.45GB/s    | 336.02GB/s    | 333.29GB/s    |
|                                               | Global Store Throughput             | 293.73GB/s    | 298.68GB/s    | 296.25GB/s    |
| **RowSumKernel**                              | Global Load Throughput              | 320.28GB/s    | 322.79GB/s    | 321.75GB/s    |
|                                               | Global Store Throughput             | 5.0044MB/s    | 5.0435MB/s    | 5.0274MB/s    |

由於只有`QKDotAndScalarKernel`用到Share Memory，因此只有該function有Share Memory throughput的數據。

---
## **3. Experiment & Analysis**
### a. System Spec
使用課程提供的Apollo GPU server
### b. Optimization
本次使用到的方法主要為share memory和Data occupancy，share memory只有針對`QKDotAndScalarKernel`做優化，以下為使用t25的測試結果：

![PO](https://hackmd.io/_uploads/rkF68aBS1e.png)

可以看到跟GPU baseline相比，share memory的優化確實加速不少，Data occupancy部分是使用`cudaOccupancyMaxPotentialBlockSize`自動找到最適合的block大小，但考量到輸入數據較大的情況，block size手動改小雖然可能造成occupancy降低，卻能提升計算效率。

### c. Others
這裡針對`QKDotAndScalarKernel`取不同的block size做測試
![BS](https://hackmd.io/_uploads/BJjxjTrHJe.png)
可以看到較大的block size反而不會造成減少計算時間，換成16\*16或8\*8後大幅提高運行時間。可能原因如前面提到的，較小的block size可讓SM同時處理較多block，犧牲一點Data Occupancy反而能換取較快的計算速度。

---
## 4. Experience & conclusion
這次的作業學到最多的地方就是嘗試各種不同block size，在理解不同大小對速度造成影響的過程中，也對Cuda如何處理Memory更熟悉，另外的難點在於share memory的優化上，很常因為改成share memory就有地方WA，需要考慮很多Data Dependency等的細節，算是一份幫助我增加Cuda programming經驗的作業。