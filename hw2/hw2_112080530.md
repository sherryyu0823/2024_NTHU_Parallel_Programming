# Parallel Programming Hw2 Report 
### 11208530余雪淩
(需要在淺色模式下閱讀，圖表的文字才看得到)
## 1. Implemention
### Pthread
首先main function會根據CPU數量創建threads，每個threads獨立執行各自的tasks。在分配threads的過程中，最直觀的方法就是將總row數除以threads的數量平分，但由於每個rows的計算時間不一，此方法會造成每個threads的工作量不平均，導致load balance效果不佳，因此在這裡採用dynamic allocation，利用全域變數t_height來紀錄threads當前計算到的row，計算完之後才會更新至下一個，如此一來較不會有load balance不均的問題，但因為這是共同變數，容易有一個以上threads同時存取的race condition發生，因此用mutex lock保護。
```c
while (t_height < height){
        int row_local;
        if (t_height >= height)
        {
            pthread_mutex_unlock(&mutex);
            break;
        }
        row_local = t_height++;
        pthread_mutex_unlock(&mutex);
// ...
```
#### Optimization
1. 避免重複計算
```
y_offset = (upper - lower) / height;
x_offset = (right - left) / width;
```
2. Vectorization
使用的QCT server支援到AVX512，可以一次執行8個double data，因此這裡直接使用AVX 512進行vectorization，實作方法主要是先把運算會用到的變數都變成<font color=#FF6600> `__m512d` </font>型態
```c
__m512d x0 = _mm512_set_pd(
    (7 < width) ? 7 * x_offset + left : 0,
    (6 < width) ? 6 * x_offset + left : 0,
    (5 < width) ? 5 * x_offset + left : 0,
    (4 < width) ? 4 * x_offset + left : 0,
    (3 < width) ? 3 * x_offset + left : 0,
    (2 < width) ? 2 * x_offset + left : 0,
    (1 < width) ? 1 * x_offset + left : 0,
    (0 < width) ? 0 * x_offset + left : 0);
__m512d y0_vec = _mm512_set1_pd(y0);
__m512d x = _mm512_setzero_pd();
__m512d y = _mm512_setzero_pd();
__m512d x2 = _mm512_mul_pd(x, x);
__m512d y2 = _mm512_mul_pd(y, y);
__m512d length_squared = _mm512_setzero_pd();
```
一開始的想法是一次直接拿8個width出來算，算完再繼續算後面8個，但仔細想會發現若某些data提早算完，會需要等帶其他data算完才能算下一個，這樣反而又降低load balance跟scalability，因此在這裡使用`idx[8]`分別紀錄每個vector當前算的位置，若算完可以直接算下一個，不必等待其他data算完
```c
int n = 8;
int idx[8] = {0, 1, 2, 3, 4, 5, 6, 7};
int iter = n - 1;
// ...
if (idx[i] < width)
{
    repeats[i]++;
    if (repeats[i] >= iters || length_squared[i] >= 4.0)
    {
        image[row_local * width + idx[i]] = repeats[i];
        idx[i] = ++iter; // process the next pixel
        if (idx[i] < width)
        {
            x0[i] = idx[i] * x_offset + left;
            x[i] = 0; y[i] = 0;
            x2[i] = 0; y2[i] = 0;
            length_squared[i] = 0;
            repeats[i] = 0;
        }
    }
}
```

### Hybrid(MPI+OpenMP)
首先利用MPI將所創建出來的processes平均分配至所有的rows，之後再利用OpenMP做Mandelbrot的運算
```c
int rows_per_proc = height / size;
int start_row = rank * rows_per_proc;
int end_row = (rank == size - 1) ? height : start_row + rows_per_proc;
mandelbrot(start_row, end_row);
```
由於每個rows的iterations次數不固定，這裡採用dynamic schedule來增加load balance
```c
#pragma omp parallel for schedule(dynamic)
    for (int row_local = start_row; row_local < end_row; row_local++){
    // ...
```
由於最後輸出的數據長度不一，因此採用可接受這種類型數據的`MPI_Gatherv`，並按照API的定義計算兩個需要傳入的array，以下為兩個參數的API定義：
> recvcounts: non-negative integer array (of length group size) containing the number of elements that are received from each process (non-negative integer) 
>  
> displs: integer array (of length group size). Entry i specifies the displacement relative to recvbuf at which to place the incoming data from process i (integer) 

```c
int *recvcounts = NULL;
int *displs = NULL;
if (rank == 0)
{
    recvcounts = (int *)malloc(size * sizeof(int));
    displs = (int *)malloc(size * sizeof(int));
    for (int i = 0; i < size; ++i)
    {
        int start = i * rows_per_proc;
        int end = (i == size - 1) ? height : start + rows_per_proc;
        recvcounts[i] = (end - start) * width;
        displs[i] = start * width;
    }
}
MPI_Gatherv(image + start_row * width, (end_row - start_row) * width, MPI_INT,
                full_image, recvcounts, displs, MPI_INT, 0, MPI_COMM_WORLD);
```
#### Optimization
這裡的加速方法跟前面Pthread版本一樣，都採用AVX512 Vectorization，一開始是直接開8個vector來計算，全部一起算完再算後面的8個，後來發現這樣做的話，如果其中有幾個先算完，會等待其他pixels一起算完才會繼續下面8個，因此一樣改成用`idx[8]`紀錄每個vector當前算的位置，算完直接算下一個後，速度有顯著提升
<font color='green'> {166.39} --> {119.94} </font>

## 2. Experiment & Analysis
### i. Methodology
#### (a) System Spec 
使用課程所提供的QCT server
#### (b) Metrics
使用Nsight system的`nvtxRangePush`和`nvtxRangePop()`來計算所需計算時間的區段，分別找出Computation time、main(overall time)、I/O time
```c
int main(int argc, char **argv)
{
    nvtxRangePush("main");
    // ...
    nvtxRangePop();
}
```

### ii. Plots: Scalability & Load Balancing & Profile
#### Pthread
##### Experimental Method
使用fast04和strict08兩筆測資，fast04有1813個iterations，strict08則有10000個，分別開了1、2、4、6、8、10 個cores進行分析
##### Performance Measurement & Analysis

###### Scalability
<div style="display: flex; align-items: center;">
    <img src="https://hackmd.io/_uploads/r1kQ7TlWyx.png" alt="fast04" width="45%" style="margin-right: 10px;">
    <img src="https://hackmd.io/_uploads/B1KCXagZke.png)" alt="strict08" width="48%">
</div>

結果顯示，隨著threads數增加，overall speedup 受到 I/O time 和 communication time 的影響，speedup 曲線趨於平緩。這一現象顯示，在平行度提高時，非計算部分（如 I/O 和通訊）的開銷佔比提升，從而形成瓶頸，使得隨著平行度的提升無法達到線性增長的加速效果。

相比之下，computation speedup 在不同測資下的表現差異顯著。在 strict08中，computation speedup 與ideal幾乎重合，表示該測試場景下的計算密度足夠高，threads花費大部分時間進行計算，I/O 和通訊的開銷可以忽略，從而達到接近理想的加速。而在 fast04 中，computation speedup 與ideal speedup 間存在較大差距，顯示出在低迭代數下計算密度較低，I/O 和通訊的開銷比例上升，限制了 speedup 增長。

總結來說，在計算密集型的測試情境（如 high-iteration 的 strict08）中，較高的計算量平攤了 I/O 和通訊的影響，使計算密度增強，並行效率接近理想值。而在低迭代數或計算量較低的情境下（如 fast04），隨著平行度增加，非計算開銷的佔比逐步增加，導致 overall speedup 衰減，並限制了scalability。

###### Load Balance
<div style="display: flex; align-items: center;">
    <img src="https://hackmd.io/_uploads/rkdRi6gZ1x.png" alt="Image 1" width="46%" style="margin-right: 10px;">
    <img src="https://hackmd.io/_uploads/ByuG2pgWyl.png" alt="LB" width="48%">
</div>

在此以8個threads為例，可以看到所有threads在兩種測資下的計算時間相差不大，可見使用的方法確實能有效提升load balance。

#### Hybrid
##### Experimental Method
使用fast06和strict08兩筆測資，fast06有5455個iterations，strict08則有10000個，固定2個nodes，分別開1、2、4、8個cores，總共有2、4、8、16個processes
##### Performance Measurement & Analysis

###### Scalability
<div style="display: flex; align-items: center;">
    <img src="https://hackmd.io/_uploads/BkWazL-bJe.png" alt="Image 1" width="46%" style="margin-right: 10px;">
    <img src="https://hackmd.io/_uploads/HJVKvLbZkg.png" alt="LB" width="48%">
</div>

在兩種測資的測試情況下，speedup表現出顯著差異。在fast06時的表現效果較好，因為iterations次數較少（5455），在運算過程中，很可能出現一個process算完但需要等其他process算完的情況，而迭代次數少有助於減少此情況發生，也減少寫入圖片的次數，平行化的開銷相對較低。然而，當迭代次數增加至10000時，過高的計算密度引發了性能瓶頸。高迭代次數下的負載不均導致部分process需要更多計算時間，從而拉低整體效率；另外，平行過程中頻繁的內存和緩存訪問競爭加劇，processes間通訊和同步的開銷在高迭代情況中也會顯著增加，進一步限制了speedup提升。因此，較多次迭代的speedup難以接近線性增加。

###### Load Balance

fast06
![LB_fast06](https://hackmd.io/_uploads/rJaOlPbZyx.png)

strict08
![LB_strict08](https://hackmd.io/_uploads/ByLihIbZke.png)

在兩種測資的表現下，各rank的執行、計算時間都非常接近，可見load balance還不錯。

![MPI](https://hackmd.io/_uploads/B1iA4vWWJx.png)
另外以fast06測試環境為例，無論有幾個processes，communication time在各rank均佔一定的時間，可見造成overall性能瓶頸的原因在這，也導致overall speedup無法線性提升

#### Optimization Strategies
雖然以上測試都顯示程式有不錯的load balance，但在scheduling上仍有進步空間，目前分配nodes的方式算是平分，也就是說每個rank計算`height/size`個rows，但每個row計算時間都不一樣，平分的話還是有可能造成各process計算時間不一，導致load balance降低或是快的等慢的情況，因此更好的作法是先讓每個node都只分配到一個row，算完後再算下一個row，這樣應該能大幅增加speedup，但由於時間因素，本次作業來不及實作此方法。

### iii. Discussion
從實驗結果中可以觀察到隨著平行度提升，Pthread 和 Hybrid（MPI + OpenMP）兩種實現方式在 Scalability 和 Load Balancing 上的表現差異。整體來說，pthread程式在高計算密度場景下（如 strict08 測資）顯示出更接近理想的 speedup，而Hybrid整體受MPI通訊時間的影響，導致 overall speedup 無法達到線性增長。

在 Scalability 部分，結果顯示 Pthread 版本的計算加速隨著 threads 增加而逐步達到瓶頸。尤其是在 fast04 測資下，由於迭代次數較低，非計算開銷佔比較高，因此 speedup 曲線趨於平緩，無法達到線性增長。相比之下，strict08 測資的計算密度更高，computation speedup 更接近理想，因為較高的迭代次數平攤了 I/O 和通訊的開銷。

Hybrid 實現中，speedup 在 fast06 測資下的表現優於 strict08 測資。由於 fast06 測資的迭代次數較低，因此 process 間的負載不均問題較少，通訊開銷對速度的影響也相對較小，導致 speedup 更接近線性增長。然而，在高迭代數的 strict08 測資中，高計算密度加劇了 memory 和 cache 的訪問競爭，process 間的通訊和同步開銷也顯著增加，最終拉低了speedup。

在 Load Balance 方面，Pthread 和 Hybrid 兩種實現均顯示出良好的負載平衡。在 8 個 threads 或多個 processes 的測試中，各個 thread 和 process 的執行時間和計算時間都相對接近，顯示出方法能有效分配計算資源，避免嚴重的負載不均問題。然而，Hybrid 實現中，隨著 process 數增加，process 間的 communication time 成為主要瓶頸，尤其在高迭代數的 strict08 測試下更為明顯。

雖然目前的分配策略已經顯示出良好的 Load Balance，但在Scheduling上仍有進一步優化的空間。目前的 row 分配策略是將 height/size 均分給每個 rank，這樣可能會因為各 row 計算時間不一致而導致某些 process 完成計算後需要等待其他 process 結束。因此，動態調度策略可以改善此問題，即每個 process 每次只分配一個 row，完成後再分配下一個，從而大幅提升整體 speedup。由於時間限制，此次實驗未能實現該方法，但未來可以進一步優化並驗證其效果。

通過比較不同場景下的 Scalability 和 Load Balancing，能夠看出高計算密度場景在 scalability 和平行效率上表現更佳，而低計算密度場景則受到通訊和 I/O 的影響顯著。進一步的優化空間在於更靈活的動態調度策略，以提升負載平衡和整體並行效率。

## 3. Experience & Conclusion
這次作業的最大收穫就是學會使用vectorization，而且此次的server能開到8個，可優化空間更大，調整方法實做出來的那刻真的感到特別有成就感，也覺得獲益良多，另外也感謝助教特別介紹Nsight system，大幅增加整理數據的方便性（雖然寫報告過程中也因為解決使用這個系統產生的bug花很多時間，好崩潰QQ），最後讓我感到遺憾的是寫報告的時候才想到更好的優化方法，來不及實作出來，希望下次作業能更精進。
